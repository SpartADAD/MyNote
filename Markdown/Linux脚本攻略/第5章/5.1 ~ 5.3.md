# 5.1　简介

​			本章会研究一些用于解析网站内容、下载数据、发送数据表单以及网站维护任务自动化之类的例子。我们可以用短短几行脚本自动化很多原本需要通过浏览器交互进行的活动。借助HTTP协议所提供的功能以及命令行实用工具,我们可以用脚本满足大量的Web自动化需求。



# 5.2	Web页面下载

## 	5.2.1	预备知识

​			wget 是一个用于文件下载的命令行工具,选项繁多且用法灵活。



## 	5.2.2	实战演练

```
wget URL
```

可以指定从多个URL处进行下载:

```
wget URL1 URL2 URL3 ..
```



## 	5.2.3　工作原理

​			下载的文件名默认和URL中的文件名会保持一致,下载日志和进度被写入 stdout 。你可以通过选项 -O 指定输出文件名。如果存在同名文件,那么该文件会被下载文件所取代:

```
wget URL -O name
```

也可以用选项 -o 指定一个日志文件,这样日志信息就不会被打印到 stdout 了。

```
wget ftp://ftp.example.com/somefile.img -O dloaded_file.img -o log
```

运行该命令,屏幕上不会出现任何内容。日志或进度信息都被写入文件log,下载文件为
dloaded_file.img。

​			由于不稳定的互联网连接,下载有可能被迫中断。选项 -t 可以指定在放弃下载之前尝试多少次:

```
wget -t 5 URL
```

**PS:将 -t 选项的值设为0会强制 wget 不断地进行重试:**



## 5.2.4 　补充内容

​			1.下载限速：

​				当下载带宽有限,却又有多个应用程序共享网络连接时,下载大文件会榨干所有的带宽,严重阻滞其他进程(可能是交互式用户)。选项 --limit-rate 可以限定下载任务能够占有的最大带宽,从而保证其他应用程序能够公平地访问Internet:

	wget--limit-rate 20k URL

在命令中可以用 k (千字节)和 m (兆字节)指定速度限制。

​			2.断点续传：

​				如果wget在下载完成之前被中断,可以利用选项 -c 从断点开始继续下载:

```
wget -c URL
```

			3.复制整个网站(镜像)
				wget 像爬虫一样也可以不在命令行中指定密码,而是在网页上手动输入密码,这就需要将 --password 改为
--ask-password 。以递归的方式遍历网页上所有的URL链接,并逐个下载。要实现这种操作,可以使用选项 --mirror :

```
wget --mirror --convert-links URL
或
wget -r -N -l -k DEPTH URL
```

选项 -l 指定页面层级(深度)。这意味着 wget 只会向下遍历指定层数的页面。该选项要与 -r(recursive,递归选项)一同使用。另外, -N 表示使用文件的时间戳。 URL 表示欲下载的网站起始地址。 -k 或 --convert-links 指示 wget 将页面的链接地址转换为本地地址。
				PS：**对网站进行镜像时,请三思而行。除非获得许可,否则你只应出于个人使用的目的才可以这么做,而且不要频繁地做镜像。**

​			4.访问需要认证的HTTP或FTP页面一些网站需要HTTP或FTP认证,可以用 --user 和 --password 提供认证信息:

	wget --user username --password pass URL
也可以不在命令行中指定密码,而是在网页上手动输入密码,这就需要将 --password 改为
--ask-password 。





# 5.3	以纯文本形式下载页面

​			Web页面其实就是包含HTML标签、JavaScript和CSS的文本文件。HTML标签定义了页面内容,如果要解析页面来查找特定的内容,这时bash就能派上用场了。可以用浏览器查看HTML文件格式是否正确,也可以用之前讲过的工具对其进行处理。 
​			解析文本文件要比解析HTML数据来得容易,因为不用再去剥离HTML标签。Lynx是一款基于命令行的Web浏览器,能够以纯文本形式下载Web网页。



## 5.3.1　预备知识

​			lynx 命令默认并没有安装在各种发行版中,不过可以通过包管理器来获取:

	yum install lynx
	或
	apt-get install lynx



## 5.3.2	实战演练

​			选项 -dump 能够以纯ASCII编码的形式下载Web页面。下面的命令可以将下载到的页面保存到文件中:

```
lynx URL -dump > webpage_as_text.txt
```

这个命令会将页面中所有的超链接( <a href="link"> )作为文本文件的页脚,单独放置在标题为 References 的区域。这样我们就可以使用正则表达式专门解析链接了。例如: